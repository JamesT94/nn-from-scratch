{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from core_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: datasets/train.csv\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def load_data(filepath):\n",
    "    df_train = pd.read_csv(filepath)\n",
    "    print('Loading data from: ' + filepath)\n",
    "\n",
    "    return df_train\n",
    "\n",
    "df_train = load_data('datasets/train.csv')\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age     Sex  Survived\n",
      "0  22.0    male         0\n",
      "1  38.0  female         1\n",
      "2  26.0  female         1\n",
      "3  35.0  female         1\n",
      "4  35.0    male         0\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[['Age', 'Sex', 'Survived']]\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Age       891 non-null    float64\n",
      " 1   Sex       891 non-null    object \n",
      " 2   Survived  891 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 21.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.fillna(0, inplace=True)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_gender(x):\n",
    "    if x == 'male':\n",
    "        return 0\n",
    "    elif x == 'female':\n",
    "        return 1\n",
    "df_train.Sex = df_train.Sex.apply(change_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 891\n",
      "df_train shape: (891, 3)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "m_train = df_train.shape[0]\n",
    "num_px = df_train.shape[1]\n",
    "\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"df_train shape: \" + str(df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train (2, 712)\n",
      "Shape of y_train (1, 712)\n",
      "Shape of X_test (2, 179)\n",
      "Shape of y_test (1, 179)\n"
     ]
    }
   ],
   "source": [
    "X = df_train.drop('Survived', axis=1).to_numpy()\n",
    "y = df_train['Survived'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.transpose()\n",
    "X_test = X_test.transpose()\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "y_train = y_train.transpose()\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "y_test = y_test.transpose()\n",
    "\n",
    "\n",
    "print(\"shape of X_train\", X_train.shape)\n",
    "print(\"Shape of y_train\", y_train.shape)\n",
    "print(\"Shape of X_test\", X_test.shape)\n",
    "print('Shape of y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [2, 15, 10, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, final_activation, learning_rate=0.0075, num_iterations=2500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialise_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters, final_activation)\n",
    "\n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches, final_activation)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 10 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.379969\n",
      "Cost after iteration 100: 0.667699\n",
      "Cost after iteration 200: 0.662965\n",
      "Cost after iteration 300: 0.665470\n",
      "Cost after iteration 400: 0.661517\n",
      "Cost after iteration 500: 0.658114\n",
      "Cost after iteration 600: 0.655887\n",
      "Cost after iteration 700: 0.650934\n",
      "Cost after iteration 800: 0.647226\n",
      "Cost after iteration 900: 0.643950\n",
      "Cost after iteration 1000: 0.639521\n",
      "Cost after iteration 1100: 0.635559\n",
      "Cost after iteration 1200: 0.632646\n",
      "Cost after iteration 1300: 0.628702\n",
      "Cost after iteration 1400: 0.629698\n",
      "Cost after iteration 1500: 0.611371\n",
      "Cost after iteration 1600: 0.618387\n",
      "Cost after iteration 1700: 0.613285\n",
      "Cost after iteration 1800: 0.607956\n",
      "Cost after iteration 1900: 0.602955\n",
      "Cost after iteration 2000: 0.598448\n",
      "Cost after iteration 2100: 0.594056\n",
      "Cost after iteration 2200: 0.589838\n",
      "Cost after iteration 2300: 0.585827\n",
      "Cost after iteration 2400: 0.581977\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, y_train, layer_dims, 'sigmoid', learning_rate=0.0075,\n",
    "                           num_iterations=2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7598314606741573\n"
     ]
    }
   ],
   "source": [
    "outputs = predict(X_train, y_train, parameters, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7653631284916201\n"
     ]
    }
   ],
   "source": [
    "outputs = predict(X_test, y_test, parameters, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in parameters.keys():\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        keys = keys + [key]*new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta, orig_parameters):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    start = 0\n",
    "    for key in orig_parameters.keys():\n",
    "#         print(key)\n",
    "#         print(orig_parameters[key])\n",
    "        size1 = orig_parameters[key].shape[0]\n",
    "        size2 = orig_parameters[key].shape[1]\n",
    "        finish = start + (size1 * size2)\n",
    "        parameters[key] = theta[start:finish].reshape((size1, size2))\n",
    "        start = finish\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in gradients.keys():\n",
    "#         print(key)\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "#         print('gradients ' + key + ': ' + str(len(gradients[key])))\n",
    "#         print(gradients[key])\n",
    "#         print('new_vector: ' + str(len(new_vector)))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    \n",
    "#         print(len(theta))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(parameters_values)                                      # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2\n",
    "        _, J_plus[i], _ = L_model_forward11(X, Y, vector_to_dictionary(thetaplus, parameters), 'sigmoid')  # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(parameters_values)                                     # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2  \n",
    "        _, J_minus[i], _ = L_model_forward11(X, Y, vector_to_dictionary(thetaminus, parameters), 'sigmoid')  # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        ### START CODE HERE ### (approx. 1 line)\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm((grad - gradapprox))                               # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                             # Step 2'\n",
    "    difference = numerator / denominator                                         # Step 3'\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward11(X, Y, parameters, final_activation):  # Successfully tested with data\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    # This is the final activation function, the desired type is defined in the variable 'final_activation'\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], final_activation)\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert (AL.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return AL, cost, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward11(AL, Y, caches, final_activation):  # Successfully tested with data\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] \\\n",
    "        = linear_activation_backward(dAL, current_cache, final_activation)\n",
    "\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA' + str(l + 1)], current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dA3', 'dW4', 'db4', 'dA2', 'dW3', 'db3', 'dA1', 'dW2', 'db2', 'dA0', 'dW1', 'db1'])\n",
      "10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (23050,1) (266,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-363-a876a12cd2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_check_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-348-acc82d60edb9>\u001b[0m in \u001b[0;36mgradient_check_n\u001b[0;34m(parameters, gradients, X, Y, epsilon)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Compare gradapprox to backward propagation gradients by computing difference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m### START CODE HERE ### (approx. 1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgradapprox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                               \u001b[0;31m# Step 1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradapprox\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# Step 2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m                                         \u001b[0;31m# Step 3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (23050,1) (266,1) "
     ]
    }
   ],
   "source": [
    "AL, cost, cache = L_model_forward11(X_train, y_train, parameters, 'sigmoid')\n",
    "gradients = L_model_backward11(AL, y_train, cache, 'sigmoid')\n",
    "print(gradients.keys())\n",
    "print(len(gradients['dW3'][0]))\n",
    "difference = gradient_check_n(parameters, gradients, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
